from langchain_ollama import ChatOllama
from langchain_core.messages import SystemMessage, HumanMessage
from ferramentas import ver_hora, abrir_programa, pesquisar_internet, monitorar_sistema, controlar_midia


print("ğŸ§  Conectando ao CÃ©rebro Local...")

sistema = SystemMessage(
  content="""
  VocÃª Ã© J.A.R.V.I.S, uma inteligÃªncia artificial avanÃ§ada criada para auxiliar Mestre Kelvin.
  Sua personalidade Ã© leal, tÃ©cnica e levemente sarcÃ¡stica (como nos filmes do Homem de Ferro).
  IMPORTANTE: Mantenha respostas curtas e objetivas (mÃ¡ximo de 3 frases), priorizando eficiÃªncia.
  NÃ£o use emojis. Aja como um sistema operacional verbal.
  """
)

llm = ChatOllama(model="llama3.2",temperature=0.5)

lista_ferramentas = [ver_hora, abrir_programa, pesquisar_internet, monitorar_sistema, controlar_midia]
llm_com_ferramentas = llm.bind_tools(lista_ferramentas)

mapa_funcoes = {
  "ver_hora": ver_hora,
  "abrir_programa": abrir_programa,
  "pesquisar_internet": pesquisar_internet,
  "monitorar_sistema": monitorar_sistema,
  "controlar_midia": controlar_midia
}

def pensar(texto_usuario):
  mensagens = [sistema, HumanMessage(content=texto_usuario)]
  resposta = llm_com_ferramentas.invoke(mensagens)

  if resposta.tool_calls:
    print(f"ğŸ”§ Jarvis solicitou: {resposta.tool_calls}")

    dados_brutos = ""

    for ferramenta in resposta.tool_calls:
      nome_ferramenta = ferramenta["name"]
      argumentos = ferramenta["args"]

      if nome_ferramenta in mapa_funcoes:
        print(f"âš™ï¸ Executando: {nome_ferramenta}...")
        funcao_real = mapa_funcoes[nome_ferramenta]
        resultado = funcao_real.invoke(argumentos)
        dados_brutos += str(resultado) + ". "

    print(f"ğŸ” Dados crus recebidos: {dados_brutos}")
    novo_prompt = f"""
        O usuÃ¡rio perguntou: '{texto_usuario}'
        A ferramenta trouxe estes dados tÃ©cnicos: {dados_brutos}
        
        MISSÃƒO: Use os dados acima para responder a pergunta do usuÃ¡rio de forma natural, falada e curta.
        NÃ£o mencione que usou ferramentas ou JSON. Apenas responda.
      """
    
    resposta_final = llm.invoke([sistema, HumanMessage(content=novo_prompt)])

    return resposta_final.content
      
  return resposta.content